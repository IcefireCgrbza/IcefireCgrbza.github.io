---
title: 1. 浅谈I/O
date: 2021-03-02 12:00:56
tags: 六、Netty
categories: 六、Netty
toc: true
---

# 一些概念

在讲I/O前，我们先讲一些操作系统的基础知识

<!-- more -->

### 用户空间与内核空间

现在操作系统都是采用虚拟存储器，以32位操作系统为例，它的寻址空间（虚拟存储空间）为4G（2^32）。操作系统的核心是内核，独立于普通的应用程序，可以访问受保护的内存空间，也有访问底层硬件设备的所有权限。为了保证用户进程不能直接操作内核（kernel），保证内核的安全，操心系统将虚拟空间划分为两部分，一部分为内核空间，一部分为用户空间。针对linux操作系统而言，将最高的1G字节（从虚拟地址0xC0000000到0xFFFFFFFF），供内核使用，称为内核空间，而将较低的3G字节（从虚拟地址0x00000000到0xBFFFFFFF），供各个进程使用，称为用户空间。

### 缓存I/O

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。

缓存 I/O 有个缺点：数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。

缓存IO对我们后面讲IO模式帮助很大

# I/O模式

对一次I/O而言，数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。也就是说，它会经历两个阶段：
1. 等待数据准备
2. 将数据从内核拷贝到进程中

正式因为这两个阶段，linux系统产生了下面五种I/O模式的方案

- 阻塞 I/O（blocking IO）
- 非阻塞 I/O（nonblocking IO）
- I/O 多路复用（ IO multiplexing）
- 信号驱动 I/O
- 异步 I/O（asynchronous IO）

注：由于signal driven IO在实际中并不常用，所以这只提及剩下的四种I/O模式

### 阻塞 I/O

进程进行系统调用后，即进入阻塞；内核开始准备数据，并将数据从内核空间拷贝到用户空间；之后，用户进程才接触阻塞，可以读取数据

![](https://icefirecgrbza.github.io/img/netty/bio.png)

### 非阻塞 I/O

进程可以设置socket为非阻塞，进行系统调用后，不会进入阻塞状态；
若内核未准备好数据，则返回一个error，进程接收到error，则会发起下一次系统调用；
若内核数据准备就绪，则将数据从内核空间拷贝到用户空间，然后返回

这种IO模式的特点就是进程需要不断轮询内核数据是否准备就绪

![](https://icefirecgrbza.github.io/img/netty/nio.png)

注意：非阻塞I/O在内核将数据从内核空间拷贝到用户空间时，是阻塞的！！！

### I/O多路复用

多路复用就是我们常说的select、poll、epoll，也称为事件驱动的IO模式，本质上是对非阻塞IO的一种封装

它的基本原理就是select、poll、epoll这些函数会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程

![](https://icefirecgrbza.github.io/img/netty/io_multiplexing.png)

### 异步 I/O

进程发起I/O操作后，可以立刻开始去做其它的事。
内核会等待数据准备完成，然后将数据从内核空间拷贝到用户空间，完成后，内核才给进程发送一个signal，告诉它I/O操作已经完成

![](https://icefirecgrbza.github.io/img/netty/aio.png)

# 系统调用

下面讲讲I/O多路复用相关的系统调用，详情参考系统调用手册：https://www.kernel.org/doc/man-pages/

### select

详情参考：https://man7.org/linux/man-pages/man2/select.2.html

```
/**
 * select支持监控多个文件描述符，阻塞知道到文件描述符就绪或超时 注意：监控的文件描述符数量最多1024个！由于调用后需要遍历文件描述符列表，所以性能较差！
 *
 * nfds：文件描述符数量
 * readfds：需要监控是否可读的文件描述符
 * writefds：需要监控是否可写的文件描述符
 * exceptfds：返回可读写的文件描述符
 * timeout：超时
 */
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);

```

### poll

详情参考：https://man7.org/linux/man-pages/man2/poll.2.html

```
struct pollfd {
    int   fd;         /* file descriptor */
    short events;     /* requested events */
    short revents;    /* returned events */
};

/**
 * poll和select没有本质的区别，调用后仍然需要遍历文件描述符列表，随着监控的文件描述符数量增长，性能线性下降
 *
 * fds：文件描述符以及需要监控的事件
 * nfds：文件描述符数量
 * timeout：超时
 */
int poll(struct pollfd *fds, nfds_t nfds, int timeout);
```

### epoll

接下来我们看看epoll，epoll分为三个系统调用

##### epoll_create

详情参考：https://man7.org/linux/man-pages/man2/epoll_create.2.html

```
/**
 * 创建一个epoll文件描述符，用于监听 需要监听读写事件的文件描述符 是否就绪
 */
int epoll_create(int size);
```

##### epoll_ctl

详情参考：https://man7.org/linux/man-pages/man2/epoll_ctl.2.html

```
/**
 * 在epoll文件描述符上注册需要监听的文件描述符
 *
 * epfd：epoll文件描述符
 * op：操作类型，增、删、改
 * fd：需要监听的文件描述符
 * event：需要监听的事件
 */
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);

struct epoll_event {
    uint32_t     events;      /* Epoll events */
    epoll_data_t data;        /* User data variable */ //调用epoll_wait后同时返回该data
};

typedef union epoll_data {
    void        *ptr;
    int          fd;
    uint32_t     u32;
    uint64_t     u64;
} epoll_data_t;
```

##### epoll_wait

详情参考：https://man7.org/linux/man-pages/man2/epoll_wait.2.html

```
/**
 * 调用epoll_wait后，阻塞直到有文件描述符就绪
 *
 * epfd：epoll文件描述符
 * events：返回已就绪的文件描述符
 * maxevents：本次调用返回的最大事件数
 * timeout：超时
 */
int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
```

##### demo

```
#define IPADDRESS   "127.0.0.1"
#define PORT        8787
#define MAXSIZE     1024
#define LISTENQ     5
#define FDSIZE      1000
#define EPOLLEVENTS 100

listenfd = socket_bind(IPADDRESS,PORT);

struct epoll_event events[EPOLLEVENTS];

//创建一个描述符
epollfd = epoll_create(FDSIZE);

//添加监听描述符事件
add_event(epollfd,listenfd,EPOLLIN);

//循环等待
for ( ; ; ){
    //该函数返回已经准备好的描述符事件数目
    ret = epoll_wait(epollfd,events,EPOLLEVENTS,-1);
    //处理接收到的连接
    handle_events(epollfd,events,ret,listenfd,buf);
}

//事件处理函数
static void handle_events(int epollfd,struct epoll_event *events,int num,int listenfd,char *buf)
{
     int i;
     int fd;
     //进行遍历;这里只要遍历已经准备好的io事件。num并不是当初epoll_create时的FDSIZE。
     for (i = 0;i < num;i++)
     {
         fd = events[i].data.fd;
        //根据描述符的类型和事件类型进行处理
         if ((fd == listenfd) &&(events[i].events & EPOLLIN))
            handle_accpet(epollfd,listenfd);
         else if (events[i].events & EPOLLIN)
            do_read(epollfd,fd,buf);
         else if (events[i].events & EPOLLOUT)
            do_write(epollfd,fd,buf);
     }
}

//添加事件
static void add_event(int epollfd,int fd,int state){
    struct epoll_event ev;
    ev.events = state;
    ev.data.fd = fd;
    epoll_ctl(epollfd,EPOLL_CTL_ADD,fd,&ev);
}

//处理接收到的连接
static void handle_accpet(int epollfd,int listenfd){
     int clifd;     
     struct sockaddr_in cliaddr;     
     socklen_t  cliaddrlen;     
     clifd = accept(listenfd,(struct sockaddr*)&cliaddr,&cliaddrlen);     
     if (clifd == -1)         
     perror("accpet error:");     
     else {         
         printf("accept a new client: %s:%d\n",inet_ntoa(cliaddr.sin_addr),cliaddr.sin_port);                       //添加一个客户描述符和事件         
         add_event(epollfd,clifd,EPOLLIN);     
     } 
}

//读处理
static void do_read(int epollfd,int fd,char *buf){
    int nread;
    nread = read(fd,buf,MAXSIZE);
    if (nread == -1)     {         
        perror("read error:");         
        close(fd); //记住close fd        
        delete_event(epollfd,fd,EPOLLIN); //删除监听 
    }
    else if (nread == 0)     {         
        fprintf(stderr,"client close.\n");
        close(fd); //记住close fd       
        delete_event(epollfd,fd,EPOLLIN); //删除监听 
    }     
    else {         
        printf("read message is : %s",buf);        
        //修改描述符对应的事件，由读改为写         
        modify_event(epollfd,fd,EPOLLOUT);     
    } 
}

//写处理
static void do_write(int epollfd,int fd,char *buf) {     
    int nwrite;     
    nwrite = write(fd,buf,strlen(buf));     
    if (nwrite == -1){         
        perror("write error:");        
        close(fd);   //记住close fd       
        delete_event(epollfd,fd,EPOLLOUT);  //删除监听    
    }else{
        modify_event(epollfd,fd,EPOLLIN); 
    }    
    memset(buf,0,MAXSIZE); 
}

//删除事件
static void delete_event(int epollfd,int fd,int state) {
    struct epoll_event ev;
    ev.events = state;
    ev.data.fd = fd;
    epoll_ctl(epollfd,EPOLL_CTL_DEL,fd,&ev);
}

//修改事件
static void modify_event(int epollfd,int fd,int state){     
    struct epoll_event ev;
    ev.events = state;
    ev.data.fd = fd;
    epoll_ctl(epollfd,EPOLL_CTL_MOD,fd,&ev);
}
```

# Java NIO编程

Api和系统调用相似，直接看demo

```
public class NioEchoServer {

    private static final int BUF_SIZE = 256;
    private static final int TIMEOUT = 3000;

    public static void main(String args[]) throws Exception {
        // 打开服务端 Socket
        ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();

        // 打开 Selector
        Selector selector = Selector.open();

        // 配置为非阻塞模式
        serverSocketChannel.configureBlocking(false);

        // 将 channel 注册到 selector 中.
        // 通常我们都是先注册一个 OP_ACCEPT 事件, 然后在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ
        // 注册到 Selector 中.
        serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT);

        // 服务端 Socket 监听8080端口
        serverSocketChannel.socket().bind(new InetSocketAddress(8080));

        while (true) {
            // 通过调用 select 方法, 阻塞地等待 channel I/O 可操作
            if (selector.select(TIMEOUT) == 0) {
                System.out.print(".");
                continue;
            }

            // 获取 I/O 操作就绪的 SelectionKey, 通过 SelectionKey 可以知道哪些 Channel 的哪类 I/O 操作已经就绪.
            Iterator<SelectionKey> keyIterator = selector.selectedKeys().iterator();

            while (keyIterator.hasNext()) {

                SelectionKey key = keyIterator.next();

                // 当获取一个 SelectionKey 后, 就要将它删除, 表示我们已经对这个 IO 事件进行了处理.
                keyIterator.remove();

                if (key.isAcceptable()) {
                    // 当 OP_ACCEPT 事件到来时, 我们就有从 ServerSocketChannel 中获取一个 SocketChannel,
                    // 代表客户端的连接
                    // 注意, 在 OP_ACCEPT 事件中, 从 key.channel() 返回的 Channel 是 ServerSocketChannel.
                    // 而在 OP_WRITE 和 OP_READ 中, 从 key.channel() 返回的是 SocketChannel.
                    SocketChannel clientChannel = ((ServerSocketChannel) key.channel()).accept();
                    clientChannel.configureBlocking(false);
                    //在 OP_ACCEPT 到来时, 再将这个 Channel 的 OP_READ 注册到 Selector 中.
                    // 注意, 这里我们如果没有设置 OP_READ 的话, 即 interest set 仍然是 OP_CONNECT 的话, 那么 select 方法会一直直接返回.
                    // 注意！！！此处可以设置attachment
                    clientChannel.register(key.selector(), OP_READ, ByteBuffer.allocate(BUF_SIZE));
                }

                if (key.isReadable()) {
                    SocketChannel clientChannel = (SocketChannel) key.channel();
                    ByteBuffer buf = (ByteBuffer) key.attachment();
                    long bytesRead = clientChannel.read(buf);
                    if (bytesRead == -1) {
                        //客户端连接关闭
                        //连接关闭会将selectionKey加入到cancelKeys
                        clientChannel.close();
                    } else if (bytesRead > 0) {
                        key.interestOps(OP_READ | SelectionKey.OP_WRITE);
                        System.out.println("Get data length: " + bytesRead);
                    }
                }

                if (key.isValid() && key.isWritable()) {
                    ByteBuffer buf = (ByteBuffer) key.attachment();
                    buf.flip();
                    SocketChannel clientChannel = (SocketChannel) key.channel();

                    clientChannel.write(buf);

                    if (!buf.hasRemaining()) {
                        key.interestOps(OP_READ);
                    }
                    buf.compact();
                }
            }
        }
    }
}
```